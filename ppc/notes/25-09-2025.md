### Parallel vs Concurrency

______________________

| Parallelism                           | Concurrency                                                     |
| ------------------------------------- | --------------------------------------------------------------- |
| Two operations at the same exact time | Two interleaving operations can be in progress at the same time |
| Multicore CPUs, GPUs, FPGAs, clusters | Any hardware                                                    |
| OpenMP, SIMD, OpenCL, CUDA            | Multiprocess, threads, actors                                   |
| Usually about performance             | Usually about correctness and modularity                        |

### Embarrassingly Parallel

___
Break down a task into smaller, **independent** sub-tasks that can be executed simultaneously. Assign work in the beginning and then collect the result in the end.  
The name comes from the fact that they are *embarrassingly* easy to parallel, with no problems of:

- race conditions
- deadlocks
- livelocks
- resource starvation
- etc.
  
How to divide work?

### Context

___
Context switching involves:
Save state -> load new state -> cache invalidation  
Costly overhead.  
Can slow down the processing if incorrectly evaluating the context (thrashing).

#### Task granularity

Fine-grained: Small chunks → Many small tasks → Better for slow/variable tasks  
Coarse-grained: Large chunks → Few large tasks → Better for fast/uniform tasks  
Trade-off between scheduling overhead and load balancing.  

Fine-grained:

- more context switching overhead.
- better load balancing.

Slow tasks → Use fine-grained (small chunks) because the overhead of creating many small tasks becomes negligible compared to the actual work time  
Fast tasks → Use coarse-grained (large chunks) because the overhead of task creation would dominate the actual work time

#### Chunk size

**Fast** tasks -> Large chunks (avoids overhead of assigning tasks)  
**Slow** tasks -> Small chunks (small assigning overhead)

#### Types of scheduling

**Static** - Work division determined at compile time  
**Dynamic** - Work distributed at runtime as threads become available  
**Guided** - Hybrid approach with decreasing chunk sizes

### Data vs Task parallelism

___
**Data** - same operation performed on different subsets of a large dataset.  
**Task** - different independent tasks performed concurrently.

**SISD** - Single Instruction Single Data  
**SIMD** - Single Instruction Multiple Data  
**MISD** - Multiple Instructions Single Data  
**MIMD** - Multiple Instructions Multiple Data

### Parallel Programming Designs

___
#### Deadlock conditions

- Mutual exclusion.  
- Hold and Wait.  
- No preemption.  
- Circular wait.  

#### Loop level parallelism

Data parallelism.  
Simple concept: do the same task in a loop until it's done.

#### Master / Worker

Task parallelism.
A Master is responsible for coordinating and assigning the N tasks to the M Workers.  
The Workers are responsible for putting their output somewhere to be collected by the Master.  
Continuous, requires explicit poison pill/shutdown via a specific message (not a task).  
Explicit need of synchronization techniques via BlockingQueue.  
Usually While-loop with queue + blocking  
**Poison Pill**: Special message (not a task) sent to workers to signal shutdown. Workers check for poison pill and terminate gracefully.  

Problems:

- Race condition
- Busy waiting

Common implementations include:

- LinkedBlockingQueue
- ArrayBlockingQueue

#### Fork / Join

Data parallelism
Divide (fork) -> Conquer (solve part of the problem) -> Combine (join)  
A thread forks it to 2 others if deemed too large (threshold) or works on it itself sequentially instead of recursively.  
Batch, ends when every future resolves.  
Synchronization implicit.  
invokeAll(), parallelStream() or futures.  
**Work Stealing**: Each thread has its own task queue. Idle threads steal tasks from busy threads' queues, providing automatic load balancing.  

Common implementations include:

- ForkJoinPool (based on M/W)

#### Scatter/Gather

Batch task parallelism  
Ends when all futures resolve  
Implicit sync via futures/barriers  
Known fixed set of tasks  
invokeAll(), parallelStream() or futures  

#### MapReduce

Chunks data -> Shuffles by key -> Reduce aggregates results

**Map** (data chunking) - divides main dataset to multiple subsets to be processed. Spread to threads.  
**Shuffling** - merges results from the map phase. Concurrency issues can and will occur.  
**Reduce** - sums all occurrences from before into a final output.

### Critical Section

___
A piece of code that accesses a shared resource (data structure, device, etc.) that must not be concurrently accessed by more than one thread of execution.

Requirements for solutions:

- Mutual exclusion (only one thread per critical section)
- Progress (if critical section is free with threads waiting, one should enter)
- Fairness (every thread should eventually get access)

### Synchronization Primitives

___

#### Atomic

Very fast, but only suitable for simple locks (**AtomicInteger**, **AtomicReference**).

#### Spinlock

Threads loop until lock is free, ultra low latency for short sections.  
Wastes CPU resources if high contention (**compareAndSet**).

#### Mutex

Thread sleeps if lock taken until notified by OS.  
General purpose, but high context-switch/syscall overhead (**synchronized**, **ReentrantLock**).  
Fair locks have a queue of sorts, while unfair locks do not.

#### Futex

Hybrid but more complex implementation of **Mutex**.  
Basis of **synchronized** and **Lock**.  
Only used on Linux, Windows uses critical sections + mutex.

#### Semaphore

Controls access to N resources via counter, but error-prone due to wrong acquire/release order (**Semaphore**).

#### Barrier

Threads wait at a point before continuing, but can stall if a thread hangs (**CyclicBarrier**)

#### CountDownLatch

One-time countdown barrier.  
Threads wait until counter reaches zero.  
Cannot be reused unlike CyclicBarrier (**CountDownLatch**).

#### ReadWriteLock

Optimized for read-heavy scenarios.  
Multiple readers can access simultaneously.  
Writers get exclusive access (**ReentrantReadWriteLock**).

#### Condition Variable

Threads wait based on condition in a critical section.  
Great for consumer-producer, but needs to be paired with mutex (**wait()** and **notify()**), misuse induces deadlocks.  
Notifying threads introduces overhead.  

#### Monitor

Encapsulates data, lock and conditions, but prone to deadlocks (**synchronized**)

#### BlockingQueue

Thread-safe queue that blocks when empty or full.  
Blocks workers trying to take from empty queue.  
Blocks producers when queue reaches capacity.  
Essential for Producer-Consumer patterns (**LinkedBlockingQueue**, **ArrayBlockingQueue**).  

#### ExecutorService

Thread pool management. Handles lifecycles and avoids explosion of threads, but pool size must be tuned (**ExecutorService**)

#### Future/CompletableFuture

Handles **async** results and task pipelines, but can get messy with deep chains (**Future**, **CompletableFuture**)

#### Stream API

Declarative parallelism, simple and handles splitting/joining, but only works on EPP problems (**parallelStream**)
