### Parallel vs Concurrency
______________________

| Parallelism                           | Concurrency                                                     |
| ------------------------------------- | --------------------------------------------------------------- |
| Two operations at the same exact time | Two interleaving operations can be in progress at the same time |
| Multicore CPUs, GPUs, FPGAs, clusters | Any hardware                                                    |
| OpenMP, SIMD, OpenCL, CUDA            | Multiprocess, threads, actors                                   |
| Usually about performance             | Usually about correctness and modularity                        |
### Embarrassingly Parallel
___
Break down a task into smaller, **independent** sub-tasks that can be executed simultaneously. Assign work in the beginning and then collect the result in the end.

The name comes from the fact that they are *embarrasingly* easy to parallel, with no problems of:
- race conditions
- deadlocks
- livelocks
- resource starvation
- etc.
How to divide work?
### Context
___
Save state -> load new state -> cache invalidation
Costly overhead.
Can slow down the processing if incorrectly evaluating the context (thrashing).
#### Task granularity
Fine-grained granularity - Small chunks -> Many small tasks
Coarse-grained granularity - Large chunks -> Few large tasks

Trade-off between scheduling overhead and load balancing

Fine-grained:
- more context switching
- less load balancing
#### Chunk size
Fast tasks -> Large chunks (avoids overhead of assigning tasks)
Slow tasks -> Small chunks (small assigning overhead)
#### Types of scheduling
Static
Dynamic
Guided
### Data vs Task parallelism
___
Data - same operation performed on different subsets of a large dataset.
Task - different independent tasks performed concurrently.

**SIMD** - Single Instruction Multiple Data
**MISD** - Multiple Instructions Single Data
**MIMD** - Multiple Instructions Multiple Data
### Parallel Programming Designs
___
#### Loop level parallelism
Data parallelism.
Simple concept: do the same task in a loop until it's done.
#### Master / Worker
Task parallelism.
A Master is responsible for coordinating and assigning the N tasks to the M Workers.
The Workers are responsible for putting their output somewhere to be collected by the Master.
Continuous, requires explicit poison pill/shutdown
Explicit need of synchronization techniques via BlockingQueue
Usually While-loop with queue + blocking

Problems:
- Race condition
- Busy waiting
#### Fork / Join
Data parallelism
Divide (fork) -> Conquer (solve part of the problem) -> Combine (join)
A thread can either finish a piece of work itself or fork it to 2 others if deemed too large (threshold)
Batch, ends when every future resolves
Synchronization implicit
invokeAll(), parallelStream() or futures
#### Scatter/Gather
Batch task parallelism
Ends when all futures resolve
Implicit sync via futures/barriers
Known fixed set of tasks
invokeAll(), parallelStream() or futures
#### MapReduce
Chunks data -> Shuffles by key -> Reduce aggregates results

### Synchronization Primitives
___ 
#### Atomic
Very fast, but only suitable for simple locks (**AtomicInteger**, **AtomicReference**)
#### Spinlock
Threads loop until lock is free, ultra low latency for short sections
Wastes CPU resources if high contention (**compareAndSet**)
#### Mutex
Thread sleeps if lock taken until notified by OS
General purpose, but high context-switch/syscall overhead (**synchronized**, **ReentrantLock**)
#### Futex
Hybrid but moree complex implementation of **Mutex**
Basis of **synchronized** and **Lock**
Only used on Linux, Windows uses critical sections + mutex
#### Semaphore
Controls access to N resources via counter, but error-prone due to wrong acquire/release order (**Semaphore**)
#### Barrier
Threads wait at a point before continuing, but can stall if a thread hangs (**CyclicBarrier**)
#### Condition Variable
Threads wait based on condition in a critical section. Great for consumer-producer, but needs to be paired with mutex (**wait()** and **notify()**), misuse induces deadlocks
#### Monitor
Encapsulates data, lock and conditions, but prone to deadlocks (**synchronized**)
#### ExecutorService
Thread pool management. Handles lifecycles and avoids explosion of threads, but pool size must be tuned (**ExecutorService**)
#### Future/CompletableFuture
Handles **async** results and task pipelines, but can get messy with deep chains (**Future**, **CompletableFuture**)
#### Stream API
Declarative parallelism, simple and handles splitting/joining, but only works on EPP problems (**parallelStream**)
